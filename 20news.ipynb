{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import metrics\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "#nltk.download('punkt')\n",
    "import numpy as np\n",
    "from bert_serving.client import BertClient\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Label: rec.autos\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])\n",
    "print()\n",
    "print(\"Label: \" + newsgroups_train.target_names[newsgroups_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TAGS = len(newsgroups_train.target_names)\n",
    "# group tags\n",
    "tags_data = [[] for i in range(NUM_TAGS)]\n",
    "for i in range(len(newsgroups_train.data)):\n",
    "    label = newsgroups_train.target[i]\n",
    "    tags_data[label].append(newsgroups_train.data[i])\n",
    "# Combine docs\n",
    "comb_tags_data = []\n",
    "for cat in range(NUM_TAGS):\n",
    "    concat_doc = \"\"\n",
    "    for doc in tags_data[cat]:\n",
    "        concat_doc += doc\n",
    "    comb_tags_data.append(concat_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates into sentences then gets the sentence embedding\n",
    "def poolEncode(doc, n=-1):\n",
    "    sentences = tokenize.sent_tokenize(doc)\n",
    "    # Maybe remove sentences that are too short\n",
    "    sentences = list(filter(lambda x : len(x) > 5, sentences))\n",
    "    # Sometimes sentences comes out empty\n",
    "    if len(sentences) == 0:\n",
    "        sentences.append(\"a\")\n",
    "        \n",
    "    sample = sentences\n",
    "    if n != -1:\n",
    "        # Only sample some sentences\n",
    "        sample = random.sample(sentences, min(n, len(sentences)))\n",
    "    doc_vecs = bc.encode(sample)\n",
    "    \n",
    "    # Mean-Pool the embeddings\n",
    "    vec = np.mean(doc_vecs, axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankScore(query, tag_vecs, tag_names, topk=1, sample=-1, v=False):\n",
    "    if sample == -1:\n",
    "        query_vec = poolEncode(query)\n",
    "    else:\n",
    "        query_vec = poolEncode(query, sample)\n",
    "    # compute normalized dot product as score\n",
    "    score = np.sum(query_vec * tag_vecs, axis=1) / np.linalg.norm(tag_vecs, axis=1)\n",
    "    topk_idx = np.argsort(score)[::-1][:topk]\n",
    "    if v:\n",
    "        for idx in topk_idx:\n",
    "            print('> %s\\t%s' % (score[idx], tag_names[idx]))\n",
    "    \n",
    "    return topk_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    tag_vecs = []\n",
    "    for i in range(NUM_TAGS):\n",
    "        tag_vecs.append(poolEncode(comb_tags_data[i], 200))\n",
    "        print(\"Loaded \" + tag_names[i] + \".\")\n",
    "    return tag_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "bc = BertClient(ip=\"10.0.0.11\")\n",
    "tag_names = newsgroups_test.target_names\n",
    "#tag_vecs = train()\n",
    "\n",
    "topk = 10\n",
    "while True:\n",
    "    query = str(input('Search: '))\n",
    "    if query == \"\":\n",
    "        continue\n",
    "    rankScore(query, tag_vecs, tag_names)\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tag_vecs):\n",
    "    preds = []\n",
    "    for query in tqdm(newsgroups_test.data[:500]):\n",
    "        preds.append(rankScore(query, tag_vecs, tag_names, sample=50, topk=10))\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tag_vecs = train()\n",
    "with open(\"tag_vecs.pkl\", \"wb\") as pickle_out:\n",
    "    pickle.dump(tag_vecs, pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds3000 = evaluate(tag_vecs)\n",
    "with open(\"preds3000.pkl\", \"wb\") as pickle_out:\n",
    "    pickle.dump(preds3000, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.42671604576131295\n",
      "accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_score: \" + str(metrics.f1_score(newsgroups_test.target[:500], preds3000[:,0], average='macro')))\n",
    "print(\"accuracy: \" + str(metrics.accuracy_score(newsgroups_test.target[:500], preds3000[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top k accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 2\n",
      "Document:\n",
      "  \n",
      "In a word, yes.\n",
      "\n",
      "Guesses: ['talk.politics.guns', 'sci.med', 'rec.motorcycles']\n",
      "Correct: alt.atheism\n",
      "\n",
      "Index: 7\n",
      "Document:\n",
      "  A friend of mine managed to get a copy of a computerised Greek and Hebrew \n",
      "Lexicon called \"The Word Perfect\" (That is not the word processing \n",
      "package WordPerfect). However, some one wiped out the EXE file, and she \n",
      "has not been able to restore it. There are no distributors of the package in \n",
      "South Africa. I would appreciate it, if some one could email me the file, or \n",
      "at least tell me where I could get it from. \n",
      "\n",
      "My email address is\n",
      "\tfortmann@superbowl.und.ac.za     or\n",
      "\tfortmann@shrike.und.ac.za\n",
      " \n",
      "Many thanks.\n",
      "Guesses: ['comp.graphics', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware']\n",
      "Correct: soc.religion.christian\n",
      "\n",
      "Top 3 accuracy: 0.702\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "preds = preds3000\n",
    "eval_size = len(preds)\n",
    "count = 0\n",
    "done = 8\n",
    "for i, y in enumerate(newsgroups_test.target[:eval_size]):\n",
    "    if y in preds[i][:k]:\n",
    "        count += 1\n",
    "    else:\n",
    "        if done < 10:\n",
    "            print('Index: ' + str(i))\n",
    "            print('Document:') \n",
    "            print('  '+newsgroups_test.data[i])\n",
    "            print(f'Guesses: {[newsgroups_test.target_names[i] for i in list(preds[i][:k])]}')\n",
    "            print(f'Correct: {newsgroups_test.target_names[y]}')\n",
    "            print()\n",
    "            done += 1\n",
    "        \n",
    "print(f'Top {k} accuracy: {count/eval_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "with open(\"preds3000.pkl\", \"rb\") as pickle_out:\n",
    "    preds3000 = pickle.load(pickle_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
